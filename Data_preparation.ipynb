{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kotug Project, Group 1\n",
    "\n",
    "Allan Guzman, June Chen, Sarah Blanc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's maybe first remember here the research questions of this project:\n",
    "\n",
    "**Main Research Question**:\n",
    "\n",
    "\"How can the reliability and efficiency of Kotug OptiPort's tugboat scheduling tool be enhanced through the development and evaluation of novel methods for predicting pickup and drop-off coordinates and other relevant variables?\"\n",
    "\n",
    "**Subresearch Questions**:\n",
    "\n",
    "- \"What advanced predictive methods can be developed and implemented to accurately forecast the coordinates (latitude and longitude) for pickup and drop-off locations of tugs on vessels in the port of Rotterdam?\"\n",
    "- \"Which variables, including AIS data, customer requirements, and port-specific constraints (or lack thereof), play a significant role in determining the pickup and drop-off locations and timing of tugs, as well as the required number of tugs for each vessel operation in the port of Rotterdam, and why are they important?\"\n",
    "- \"Which of the above mentioned variables can be combined in order to increase the reliability of the results, and why can they be combined?\"\n",
    "\n",
    "The work on this first research question in done in the file called \"Prediction\", while the 2 other subquestions are answered in this file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA PROCESSING** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data processing refers to the transformation of raw data into meaningful and actionable insights. It includes a set of procedures that transform data into a form that can be used for analysis, interpretation, and making decisions. This vital procedure is essential for a number of reasons, all of which support the overall effectiveness and success of this data science project.\n",
    "1. Extracting insights from our raw data: in its purest form, our raw data lacks organisation and clarity. In order to make sure the data is reliable and consistent, it must be cleaned, and arranged. This conversion makes it easier to extract insightful information that may be applied to a our forecasting method.\n",
    "2. Enhancing data quality: it helps in identifying and rectifying errors, inconsistencies, and inaccuracies within our data. We may make decisions based on trustworthy information and achieve better results and more operational efficiency.\n",
    "3. Facilitating data integration: in this project, we work with data from several sources. data processing allows us to harmonise and integrate our data into a single format. \n",
    "\n",
    "In this file, you can aslo find visualizations of the Rotterdam DataSet, especially the port and the method with the squares (dsicretization) that we decide to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the diagonal length for each grid cell in meters (500 meters)\n",
    "diagonal_length_meters = 1000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of the librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from ipyleaflet import Map, GeoJSON, GeoData, basemaps, LayersControl\n",
    "from shapely.geometry import Point, Polygon, LineString\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import folium\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "from rasterio.plot import reshape_as_image \n",
    "from rasterio.plot import reshape_as_raster\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.spatial import KDTree\n",
    "import numpy as np\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "from shapely.ops import nearest_points\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "from scipy.interpolate import BSpline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Historic Towages Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('Data/Input_EDA/historic_towages.json')\n",
    "#f = open('Data/Input_EDA/historic_towages.json')\n",
    " \n",
    "# returns JSON object as a dictionary\n",
    "tow_data = json.load(f)\n",
    " \n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Processing of the grid/the water surface in the port of Rotterdam**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of the water, within the Port of Rotterdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GeoJSON data from your file\n",
    "file_path = 'Data/Input_EDA/haven.txt'\n",
    "#file_path = 'Data/Input_EDA/haven.txt'\n",
    "gdf_haven = gpd.read_file(file_path)\n",
    "gdf_haven.dropna(axis=1)\n",
    "\n",
    "# Plot the GeoDataFrame\n",
    "gdf_haven.plot()\n",
    "plt.title('GeoJSON Data Visualization')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of the polygons representing the in-port water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map to visualize the areas\n",
    "poly_map = folium.Map(\n",
    "    location=[51.9, 4.2],    \n",
    "    zoom_start=11,\n",
    "    \n",
    ")\n",
    "gjson = folium.features.GeoJson(gdf_haven).add_to(poly_map)\n",
    "\n",
    "poly_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge of all the polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge of the polygons\n",
    "merged_haven = unary_union(gdf_haven['geometry'])\n",
    "\n",
    "poly_map = folium.Map(\n",
    "    location=[51.9, 4.2],    \n",
    "    zoom_start=11\n",
    ")\n",
    "gjson = folium.features.GeoJson(merged_haven).add_to(poly_map)\n",
    "\n",
    "poly_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the preceding map, the boundaries are somewhat ambiguous, with a substantial portion of land appearing as water (a closer look will reveal this). Consequently, we made the determination to seek more accurate data. A team member took the initiative to meticulously delineate all the coastlines within the Port of Rotterdam manually. This endeavor has enabled us to establish more accurate distinctions for the water boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import waterline from NL \n",
    "gdf_coast = gpd.read_file('Data/Input_EDA/NL_Waterline.gml', driver='GML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom coordinate system (modify this with your actual custom CRS)\n",
    "input_crs = \"EPSG:28992\"\n",
    "\n",
    "# Define the target EPSG:4326 CRS\n",
    "target_crs = \"EPSG:4326\"\n",
    "\n",
    "# Create a GeoDataFrame and set the custom CRS\n",
    "gdf_coast.set_crs(input_crs, allow_override=True, inplace=True)\n",
    "\n",
    "# Transform the geometries to EPSG:4326\n",
    "gdf_coast = gdf_coast.to_crs(target_crs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Point(51.91865, 3.92079)\n",
    "p2 = Point(51.99519, 3.89333)\n",
    "p3 = Point(52.0611, 4.01894)\n",
    "p4 = Point(51.93935, 4.29704)\n",
    "p5 = Point(51.94185, 4.53538)\n",
    "p6 = Point(51.84609, 4.5546)\n",
    "p7 = Point(51.81044, 4.28269)\n",
    "p8 = Point(51.88806, 4.14468)\n",
    "points = [p1, p2, p3, p4, p5, p6, p7, p8, p1]\n",
    "\n",
    "big_area = Polygon([[p.y, p.x] for p in points])\n",
    "big_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heaven_and_sea = unary_union((merged_haven,gdf_coast.head(1)['geometry'][0]))\n",
    "polygon_gdf = gpd.GeoDataFrame(geometry=[heaven_and_sea], crs=gdf_coast.crs)\n",
    "intersection = gpd.clip(gdf_coast, polygon_gdf)\n",
    "intersection = intersection[(intersection.geom_type == 'Polygon') | (intersection.geom_type == 'MultiPolygon')]\n",
    "intersection = intersection[intersection.functie != 'waterzuivering']\n",
    "intersection = intersection[(intersection.typeWater == 'waterloop') | (intersection.typeWater == 'zee')]\n",
    "geometry_counts = intersection['typeWater'].value_counts()\n",
    "\n",
    "intersection_union = unary_union(intersection['geometry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our proposed approach entails delineating the polygon encapsulating the entire water expanse within the Port of Rotterdam into discrete smaller squares. The modification of the square size is a critical parameter (\"diagonal_length_meters\") that requires careful evaluation of the available processing resources. It is crucial to find the ideal balance between computational viability and accuracy. Although increasing the number of smaller squares improves the discretization's granularity, it is crucial to recognise the computing limitations. An overabundance of detail could impose a strain on the computational infrastructure and hinder the effectiveness of the model. Therefore, it's crucial to make a wise trade-off that balances our available computational resources with precision's smooth integration into the predictive model. \n",
    "You can find below the process of discretizing the Polygone that we just created with the new and better coastal boarders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original polygon\n",
    "original_polygon = gpd.GeoDataFrame(geometry=[big_area], crs=gdf_coast.crs)\n",
    "\n",
    "# Define the diagonal length for each grid cell in meters (500 meters)\n",
    "\n",
    "# Convert diagonal length from meters to degrees\n",
    "def meters_to_degrees(meters):\n",
    "    # Earth's radius in meters (approximately)\n",
    "    earth_radius_meters = 6371000.0\n",
    "    # Calculate the conversion factor from meters to degrees\n",
    "    degrees_per_radian = 180.0 / np.pi\n",
    "    radians_per_degree = np.pi / 180.0\n",
    "    return (meters / earth_radius_meters) * degrees_per_radian\n",
    "\n",
    "diagonal_length_degrees = meters_to_degrees(diagonal_length_meters)\n",
    "\n",
    "# Calculate the size of each grid cell\n",
    "cell_size = diagonal_length_degrees / np.sqrt(2)\n",
    "\n",
    "# Get the bounding box of the original polygon\n",
    "bbox = original_polygon.total_bounds  # Returns [minx, miny, maxx, maxy]\n",
    "\n",
    "# Create a grid of points within the bounding box\n",
    "x_min, y_min, x_max, y_max = bbox\n",
    "x_coords = np.arange(x_min, x_max, cell_size)\n",
    "y_coords = np.arange(y_min, y_max, cell_size)\n",
    "\n",
    "# Create a list of square polygons as Shapely Polygons\n",
    "square_polygons = [Polygon([(x, y), (x + cell_size, y), (x + cell_size, y + cell_size), (x, y + cell_size)]) for x in x_coords for y in y_coords]\n",
    "\n",
    "# Create a GeoDataFrame from the square polygons\n",
    "grid_gdf = gpd.GeoDataFrame(geometry=square_polygons, crs=original_polygon.crs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the grid polygons to keep only those within the original polygon\n",
    "grid_within_polygon = grid_gdf[grid_gdf.intersects(original_polygon.unary_union)]\n",
    "grid_within_polygon.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_within_polygon = gpd.clip(grid_within_polygon, intersection)\n",
    "grid_within_polygon.reset_index(inplace = True)\n",
    "grid_within_polygon.rename(columns={\"index\": \"area_ID\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_within_polygonbis = grid_within_polygon.copy()\n",
    "grid_within_polygonbis['center'] = grid_within_polygonbis['geometry'].centroid\n",
    "grid_within_polygonbis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final grid can be visualized below. This is the grid that is going to be used for the predictions method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_map = folium.Map(\n",
    "    location=[51.9, 4.2],    \n",
    "    zoom_start=11\n",
    ")\n",
    "\n",
    "folium.features.GeoJson(grid_within_polygon).add_to(poly_map) \n",
    "\n",
    "poly_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Processing: the tugs and vessels information**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This stage holds paramount importance for the progression of the project. Here, our main goal is to carefully clean and prepare the several dataframes that are needed in order to use the prediction techniques. It is necessary to recognise that the data that has been supplied is, in its natural state, inadequate and inappropriate for our needs at this time. A great deal of work has gone into fixing the numerous problems that the datasets include. Given the sheer enormity of some datasets (ais data sets), a specialized tool in the realm of data science was employed to streamline part of the data processing. This concerns the data about the vessels (dimensions: lenght and width, draught, and type). This tool, called Alteryx Designer, proved instrumental in handling the intricacies of large datasets efficiently. Indeed, Alteryx Designer is a powerful data analytics and visualization tool that allows users to prepare, blend, and analyze data quickly and efficiently. The decision to opt for this tool was driven by the extensive time required to load the data for each month when utilizing Python on our computers. Due to this extended period, we decided to utilise the knowledge of a team member who had previously worked with Alteryx Designer.\n",
    "\n",
    "Furthermore, it is important to mention that after a discussion with the employees of Kotug, only the first pick-up and the last drop-off locations have to be predicted. This means that if many tugs help a vessel, only the locations of the first one that pick-up and the last one to drop-off will be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store tugs data and their original JSON index\n",
    "tugs_data = []\n",
    "trip_ID = []\n",
    "vessel_name = []  # Create a list to store previous vessel names\n",
    "vessel_mmsi = []  # Create a list to store previous vessel names\n",
    "\n",
    "\n",
    "# Extract tugs data into a separate list, store their original index, and the previous vessel name\n",
    "for i, item in enumerate(tow_data):\n",
    "    for tug in item['tugs']:\n",
    "        tugs_data.append(tug)\n",
    "        trip_ID.append(i)\n",
    "        vessel_name.append(item['vessel']['name'])\n",
    "        vessel_mmsi.append(item['vessel']['mmsi'])\n",
    "\n",
    "\n",
    "# Create a DataFrame for tugs data with the original index and previous vessel name\n",
    "tugs_df = pd.DataFrame(tugs_data)\n",
    "tugs_df['trip_ID'] = trip_ID  # Add original index as a new column\n",
    "tugs_df['vessel_name'] = vessel_name  # Add vessel name as a new column\n",
    "tugs_df['vessel_mmsi'] = vessel_mmsi\n",
    "\n",
    "tugs_df['from'] = pd.to_datetime(tugs_df['from'],format='ISO8601')\n",
    "tugs_df['to'] = pd.to_datetime(tugs_df['to'],format='ISO8601')\n",
    "\n",
    "tugs_df['from_rounded'] = tugs_df['from'].dt.round('H')\n",
    "tugs_df['to_rounded'] = tugs_df['to'].dt.round('H')\n",
    "\n",
    "# Create a DataFrame for the rest of the data (excluding tugs)\n",
    "vessel_info = [{'from': item['from'],\n",
    "               'to': item['to'],\n",
    "               'vessel': item['vessel'],\n",
    "               'type': item['type'],\n",
    "               'additional_data': item['additional_data']}\n",
    "              for item in tow_data]\n",
    "\n",
    "vessel_df = pd.DataFrame(vessel_info)\n",
    "\n",
    "tugs_df = tugs_df.merge(vessel_df['type'], left_on='trip_ID', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tugs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition of the weather data (direction of the wind and wind speed)\n",
    "We categorized them in excel, in order to be able to use them in the prediction model. We chose to have 8 categories for the wind speed (N, NE, E, SE, S, SW, W, NW) and 4 categories for the wind speed (Low, Mid, High, Extreme). We collected the data for the 31st of May 2022, till the 2nd of June 2023. This idea here was to take a little buffer margin on the time intervall, so we were sure to get at least all the data needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind = pd.read_csv('Data/Input_EDA/Weather.csv')\n",
    "wind['Datetime'] =  pd.to_datetime(wind['Datetime'])\n",
    "wind.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vessel data information**\n",
    "\n",
    "Here you can see the csv files that have been imported from the special tool, after processing the data there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_size_data = pd.read_csv('Data/Input_EDA/Vessel_size_data.csv')\n",
    "\n",
    "vessel_draught_data = pd.read_csv('Data/Input_EDA/Vessel-draught_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_draught_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_draught_data.dropna(inplace= True)\n",
    "\n",
    "vessel_draught_data['device_mmsi'] = vessel_draught_data['device_mmsi'].astype(int)\n",
    "vessel_draught_data['DateTime'] = vessel_draught_data['DateTime'].str.replace('\"', '')\n",
    "\n",
    "vessel_draught_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_draught_data['DateTime'] = pd.to_datetime(vessel_draught_data['DateTime'], format='ISO8601')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We opted to handle incoming and departing vessels as distinct cases, primarily because the necessary variables vary based on the scenario. For incoming vessels, the pertinent variables include \"from\" (excluding the haven, where the pertinent variable is \"to_haven\"), whereas for departing vessels, the relevant variables are \"to\" (excluding the haven, where the pertinent variable is \"from_haven\"). Consequently, we established two separate data frames, each exclusively containing the pertinent data corresponding to its respective case (incoming or leaving)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incoming vessels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tugs_incoming = tugs_df[tugs_df['type']=='incoming']\n",
    "tugs_incoming = tugs_incoming[['from','from_rounded', 'from_location' ,'to_haven', 'trip_ID', 'vessel_mmsi']]\n",
    "\n",
    "# Create a Shapely Point geometry from the \"from_location\" coordinates\n",
    "tugs_incoming['geometry'] = tugs_incoming['from_location'].apply(lambda coord: Point(coord))\n",
    "tugs_incoming['from_location'] = tugs_incoming['from_location'].apply(lambda coord: Point(coord))\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "tugs_incoming_gdf = gpd.GeoDataFrame(tugs_incoming, geometry='geometry')\n",
    "tugs_incoming_gdf = tugs_incoming_gdf.set_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_data = gpd.sjoin(grid_within_polygonbis, tugs_incoming_gdf, how=\"inner\", op=\"intersects\")\n",
    "incoming_data.sort_values('from', inplace = True)\n",
    "incoming_data.drop_duplicates(subset = 'trip_ID',keep='first', inplace = True)\n",
    "\n",
    "incoming_data = incoming_data.merge(wind[['Datetime', 'Wind_Direction_Cat1','Wind_Direction_Cat2', 'Wind_Speed']], left_on = 'from_rounded', right_on = 'Datetime', how = 'left')\n",
    "incoming_data.drop(columns = ['Datetime','index_right'], inplace= True)\n",
    "#incoming_data.to_csv('Data/Input_ML/incoming_weather.csv')\n",
    "\n",
    "incoming_data = incoming_data.merge(vessel_size_data, left_on = 'vessel_mmsi', right_on = 'device_mmsi', how = 'inner')\n",
    "incoming_data.drop(columns = ['device_mmsi'], inplace= True)\n",
    "#incoming_data.to_csv('Data/Input_ML/incoming_weather_size.csv')\n",
    "\n",
    "incoming_data = incoming_data.merge(vessel_draught_data, left_on = ['vessel_mmsi','from_rounded'], right_on = ['device_mmsi','DateTime'], how = 'inner')\n",
    "incoming_data.drop(columns = ['device_mmsi', 'DateTime'], inplace= True)\n",
    "#incoming_data.to_csv('Data/Input_ML/incoming_weather_size_draught.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaving vessels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter fot leaving\n",
    "tugs_leaving = tugs_df[tugs_df['type']=='leaving']\n",
    "tugs_leaving = tugs_leaving[['to','to_rounded',\t'to_location' ,'from_haven', 'trip_ID', 'vessel_mmsi']]\n",
    "\n",
    "# Create a Shapely Point geometry from the \"from_location\" coordinates\n",
    "tugs_leaving['geometry'] = tugs_leaving['to_location'].apply(lambda coord: Point(coord))\n",
    "tugs_leaving['to_location'] = tugs_leaving['to_location'].apply(lambda coord: Point(coord))\n",
    "\n",
    "# Convert the DataFrame to a GeoDataFrame\n",
    "tugs_leaving_gdf = gpd.GeoDataFrame(tugs_leaving, geometry='geometry')\n",
    "tugs_leaving_gdf = tugs_leaving_gdf.set_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaving_data = gpd.sjoin(grid_within_polygonbis, tugs_leaving_gdf, how=\"inner\", op=\"intersects\")\n",
    "leaving_data.sort_values('to', inplace = True)\n",
    "leaving_data.drop_duplicates(subset = 'trip_ID',keep='last', inplace = True)\n",
    "\n",
    "leaving_data = leaving_data.merge(wind[['Datetime', 'Wind_Direction_Cat1','Wind_Direction_Cat2', 'Wind_Speed']], left_on = 'to_rounded', right_on = 'Datetime', how = 'left')\n",
    "leaving_data.drop(columns = ['Datetime','index_right'], inplace= True)\n",
    "#leaving_data.to_csv('Data/Input_ML/leaving_weather.csv')\n",
    "\n",
    "leaving_data = leaving_data.merge(vessel_size_data, left_on = 'vessel_mmsi', right_on = 'device_mmsi', how = 'inner')\n",
    "leaving_data.drop(columns = ['device_mmsi'], inplace= True)\n",
    "#leaving_data.to_csv('Data/Input_ML/leaving_weather_size.csv')\n",
    "\n",
    "leaving_data = leaving_data.merge(vessel_draught_data, left_on = ['vessel_mmsi','to_rounded'], right_on = ['device_mmsi','DateTime'], how = 'inner')\n",
    "leaving_data.drop(columns = ['device_mmsi', 'DateTime'], inplace= True)\n",
    "#leaving_data.to_csv('Data/Input_ML/leaving_weather_size_draught.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haven_locations = tugs_df[tugs_df['type'] == 'incoming']\n",
    "haven_locations = haven_locations[['to_location','to_haven']]\n",
    "haven_locations = haven_locations.dropna(subset=['to_haven']).reset_index().drop(columns='index')\n",
    "haven_locations[['long','lat']] = pd.DataFrame(haven_locations['to_location'].to_list(), columns=['latitude', 'longitude'])\n",
    "haven_locations.drop(columns='to_location',inplace=True)\n",
    "haven_locations = haven_locations.groupby(\"to_haven\").mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tugs = pd.concat([tugs_incoming_gdf,tugs_leaving_gdf])\n",
    "\n",
    "all_tugs['haven'] = all_tugs['to_haven'].combine_first(all_tugs['from_haven'])\n",
    "\n",
    "top_10_haven = pd.DataFrame(all_tugs['haven'].value_counts()[:10]).reset_index()\n",
    "\n",
    "haven_locations = haven_locations.loc[top_10_haven['haven']]\n",
    "\n",
    "for_line = all_tugs[all_tugs['haven'].isin(top_10_haven['haven'])]\n",
    "\n",
    "for_line = for_line[['trip_ID',\t'vessel_mmsi', 'geometry','haven']].groupby(by = 'haven').apply(lambda x: x)\n",
    "\n",
    "#for_line = for_line.loc[selected_haven]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (K) - You can determine this using the Elbow Method or Silhouette Score\n",
    "num_clusters = 20  # Adjust as needed\n",
    "\n",
    "# Convert the Point geometries to a NumPy array for clustering\n",
    "points_array = [point.coords[0] for point in for_line['geometry']]\n",
    "points_array = np.array(points_array)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters, init='k-means++')\n",
    "for_line['cluster'] = kmeans.fit_predict(points_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = for_line['cluster'].value_counts()\n",
    "cluster_counts = pd.DataFrame(cluster_counts)\n",
    "cluster_counts = cluster_counts[cluster_counts['count'] >= float(cluster_counts.sum()*0.01)].reset_index()\n",
    "\n",
    "for_line = for_line[for_line['cluster'].isin(cluster_counts['cluster'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = kmeans.cluster_centers_ \n",
    "centroids_gdf = [Point(x, y) for x, y in centroids]\n",
    "centroids_gdf = gpd.GeoDataFrame(geometry=centroids_gdf, crs=target_crs).reset_index()\n",
    "centroids_gdf = centroids_gdf[centroids_gdf['index'].isin(cluster_counts['cluster'])]\n",
    "\n",
    "# Transform the points to a projected coordinate system (e.g., EPSG 3857 for meters)\n",
    "centroids_gdf_n = centroids_gdf.to_crs(epsg=3857)\n",
    "\n",
    "# Create a KD-Tree for efficient nearest neighbor search\n",
    "tree = KDTree(centroids_gdf_n.geometry.apply(lambda geom: (geom.x, geom.y)).tolist())\n",
    "\n",
    "# Set your threshold distance (in the same unit as the CRS)\n",
    "threshold_distance = 5000  # Replace with your desired threshold distance\n",
    "\n",
    "# Function to find the closest neighbor's distance\n",
    "def find_closest_distance(index, tree):\n",
    "    distances, _ = tree.query([centroids_gdf_n.geometry.iloc[index].x, centroids_gdf_n.geometry.iloc[index].y], k=2)\n",
    "    return distances[1]  # The closest neighbor is at index 0, so we take the second closest\n",
    "\n",
    "# Calculate the closest neighbor distances for each point\n",
    "centroids_gdf_n['closest_distance'] = [find_closest_distance(i, tree) for i in range(len(centroids_gdf_n))]\n",
    "\n",
    "# Filter the points based on the threshold distance\n",
    "filtered_gdf = centroids_gdf_n[centroids_gdf_n['closest_distance'] <= threshold_distance]\n",
    "\n",
    "# Remove the 'closest_distance' column if you don't need it anymore\n",
    "filtered_gdf = filtered_gdf.drop(columns=['closest_distance'])\n",
    "\n",
    "# Now, 'filtered_gdf' contains only the points whose closest neighbor is within the threshold distance.\n",
    "\n",
    "filtered_gdf.to_crs(target_crs, inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_map = folium.Map(\n",
    "    location=[51.9, 4.2],    \n",
    "    zoom_start=11\n",
    ")\n",
    "\n",
    "folium.features.GeoJson(filtered_gdf).add_to(poly_map) \n",
    "\n",
    "poly_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor_tsp_gdf(gdf, start, end):\n",
    "    unvisited = set(gdf.geometry)\n",
    "    path = [start]\n",
    "    unvisited.remove(start)\n",
    "\n",
    "    while unvisited:\n",
    "        nearest_point = min(unvisited, key=lambda point: point.distance(path[-1]))\n",
    "        path.append(nearest_point)\n",
    "        unvisited.remove(nearest_point)\n",
    "\n",
    "    # After constructing a path, add the end point\n",
    "    path.append(end)\n",
    "\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define start and end points as Shapely Point objects\n",
    "haven_point = Point(haven_locations.loc[selected_haven]['long'],haven_locations.loc[selected_haven]['lat'])\n",
    "sea_point = Point(4.069379, 52.032103)\n",
    "\n",
    "\n",
    "# Calculate the distance from each point in the GeoDataFrame to the given points\n",
    "filtered_gdf['distance_to_haven'] = filtered_gdf.geometry.apply(lambda geom: geom.distance(haven_point))\n",
    "filtered_gdf['distance_to_sea'] = filtered_gdf.geometry.apply(lambda geom: geom.distance(sea_point))\n",
    "\n",
    "# Find the index of the point with the minimum distance to the given points\n",
    "nearest_haven_index = filtered_gdf['distance_to_haven'].idxmin()\n",
    "nearest_sea_index = filtered_gdf['distance_to_sea'].idxmin()\n",
    "\n",
    "# Get the actual nearest points\n",
    "nearest_haven_point = filtered_gdf.at[nearest_haven_index, 'geometry']\n",
    "nearest_sea_point = filtered_gdf.at[nearest_sea_index, 'geometry']\n",
    "\n",
    "print(nearest_haven_point)\n",
    "print(nearest_sea_point)\n",
    "\n",
    "# Find the shortest path\n",
    "shortest_path = nearest_neighbor_tsp_gdf(filtered_gdf,nearest_haven_point, nearest_sea_point)\n",
    "#print(shortest_path)\n",
    "\n",
    "path = gpd.GeoDataFrame(shortest_path, columns = ['geometry'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom function to create LineString geometries from a list of Point geometries\n",
    "def create_line_from_points(point_list):\n",
    "    if len(point_list) < 2:\n",
    "        return None\n",
    "    return LineString(point_list)\n",
    "\n",
    "# Create a new GeoDataFrame to store the lines\n",
    "lines_gdf = gpd.GeoDataFrame()\n",
    "\n",
    "# Apply the custom function to create lines\n",
    "lines_gdf['geometry'] = path.apply(create_line_from_points)\n",
    "\n",
    "# Plot the lines\n",
    "lines_gdf.set_crs(target_crs, inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_map = folium.Map(\n",
    "    location=[51.9, 4.2],    \n",
    "    zoom_start=11\n",
    ")\n",
    "\n",
    "folium.features.GeoJson(lines_gdf).add_to(poly_map) \n",
    "#folium.features.GeoJson(for_line['geometry']).add_to(poly_map) \n",
    "\n",
    "poly_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor_tsp_gdf(gdf, start, end):\n",
    "        unvisited = set(gdf.geometry)\n",
    "        path = [start]\n",
    "        unvisited.remove(start)\n",
    "\n",
    "        while unvisited:\n",
    "            nearest_point = min(unvisited, key=lambda point: point.distance(path[-1]))\n",
    "            path.append(nearest_point)\n",
    "            unvisited.remove(nearest_point)\n",
    "\n",
    "        # After constructing a path, add the end point\n",
    "        path.append(end)\n",
    "\n",
    "        return path\n",
    "\n",
    "\n",
    "def trayectory_line(gdf, haven, K, d, haven_locations):\n",
    "    gdf_fx = gdf.copy()\n",
    "    gdf_fx = gdf_fx.loc[haven]\n",
    "\n",
    "    # Number of clusters (K) - You can determine this using the Elbow Method or Silhouette Score\n",
    "    num_clusters = K  # Adjust as needed\n",
    "\n",
    "    # Convert the Point geometries to a NumPy array for clustering\n",
    "    points_array = [point.coords[0] for point in gdf_fx['geometry']]\n",
    "    points_array = np.array(points_array)\n",
    "\n",
    "    # Apply K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, init='k-means++')\n",
    "    gdf_fx['cluster'] = kmeans.fit_predict(points_array)\n",
    "\n",
    "    cluster_counts = gdf_fx['cluster'].value_counts()\n",
    "    cluster_counts = pd.DataFrame(cluster_counts)\n",
    "    cluster_counts = cluster_counts[cluster_counts['count'] >= float(cluster_counts.sum()*0.015)].reset_index()\n",
    "\n",
    "    gdf_fx = gdf_fx[gdf_fx['cluster'].isin(cluster_counts['cluster'])]\n",
    "\n",
    "\n",
    "    centroids = kmeans.cluster_centers_ \n",
    "    centroids_gdf = [Point(x, y) for x, y in centroids]\n",
    "    centroids_gdf = gpd.GeoDataFrame(geometry=centroids_gdf, crs=target_crs) \n",
    "    centroids_gdf = centroids_gdf.loc[cluster_counts['cluster']]\n",
    "\n",
    "    # Transform the points to a projected coordinate system (e.g., EPSG 3857 for meters)\n",
    "    centroids_gdf_n = centroids_gdf.to_crs(epsg=3857)\n",
    "\n",
    "    # Create a KD-Tree for efficient nearest neighbor search\n",
    "    tree = KDTree(centroids_gdf_n.geometry.apply(lambda geom: (geom.x, geom.y)).tolist())\n",
    "\n",
    "    # Set your threshold distance (in the same unit as the CRS)\n",
    "    threshold_distance = d  # Replace with your desired threshold distance\n",
    "\n",
    "    # Function to find the closest neighbor's distance\n",
    "    def find_closest_distance(index, tree):\n",
    "        distances, _ = tree.query([centroids_gdf_n.geometry.iloc[index].x, centroids_gdf_n.geometry.iloc[index].y], k=2)\n",
    "        return distances[1]  # The closest neighbor is at index 0, so we take the second closest\n",
    "\n",
    "    # Calculate the closest neighbor distances for each point\n",
    "    centroids_gdf_n['closest_distance'] = [find_closest_distance(i, tree) for i in range(len(centroids_gdf_n))]\n",
    "\n",
    "    # Filter the points based on the threshold distance\n",
    "    filtered_gdf = centroids_gdf_n[centroids_gdf_n['closest_distance'] <= threshold_distance]\n",
    "\n",
    "    # Remove the 'closest_distance' column if you don't need it anymore\n",
    "    filtered_gdf = filtered_gdf.drop(columns=['closest_distance'])\n",
    "\n",
    "    # Now, 'filtered_gdf' contains only the points whose closest neighbor is within the threshold distance.\n",
    "\n",
    "    filtered_gdf.to_crs(target_crs, inplace= True)\n",
    "\n",
    "    # Define start and end points as Shapely Point objects\n",
    "    haven_point = Point(haven_locations.loc[haven]['long'],haven_locations.loc[haven]['lat'])\n",
    "    sea_point = Point(4.063312, 52.012083)\n",
    "    \n",
    "\n",
    "\n",
    "    # Calculate the distance from each point in the GeoDataFrame to the given points\n",
    "    filtered_gdf['distance_to_haven'] = filtered_gdf.geometry.apply(lambda geom: geom.distance(haven_point))\n",
    "    filtered_gdf['distance_to_sea'] = filtered_gdf.geometry.apply(lambda geom: geom.distance(sea_point))\n",
    "\n",
    "    # Find the index of the point with the minimum distance to the given points\n",
    "    nearest_haven_index = filtered_gdf['distance_to_haven'].idxmin()\n",
    "    nearest_sea_index = filtered_gdf['distance_to_sea'].idxmin()\n",
    "\n",
    "    # Get the actual nearest points\n",
    "    nearest_haven_point = filtered_gdf.at[nearest_haven_index, 'geometry']\n",
    "    nearest_sea_point = filtered_gdf.at[nearest_sea_index, 'geometry']\n",
    "\n",
    "    # Find the shortest path\n",
    "    shortest_path = nearest_neighbor_tsp_gdf(filtered_gdf,nearest_haven_point, nearest_sea_point)\n",
    "    shortest_path.insert(0, haven_point)\n",
    "    #print(shortest_path)\n",
    "\n",
    "    path = gpd.GeoDataFrame(shortest_path, columns = ['geometry'] )\n",
    "    # Create a custom function to create LineString geometries from a list of Point geometries\n",
    "    def create_line_from_points(point_list):\n",
    "        if len(point_list) < 2:\n",
    "            return None\n",
    "        return LineString(point_list)\n",
    "\n",
    "    # Create a new GeoDataFrame to store the lines\n",
    "    lines_gdf = gpd.GeoDataFrame()\n",
    "\n",
    "    # Apply the custom function to create lines\n",
    "    lines_gdf['geometry'] = path.apply(create_line_from_points)\n",
    "    lines_gdf['haven'] = haven\n",
    "\n",
    "    # Plot the lines\n",
    "    lines_gdf.set_crs(target_crs, inplace= True)\n",
    "\n",
    "    return lines_gdf, shortest_path#, centroids_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trayectories = []\n",
    "for haven in top_10_haven['haven']:\n",
    "    trayectory, a = trayectory_line(for_line, haven, 20, 15000, haven_locations)\n",
    "    trayectories.append(trayectory)\n",
    "\n",
    "trayectories = gpd.GeoDataFrame(pd.concat(trayectories, ignore_index=True), crs=trayectories[0].crs)\n",
    "\n",
    "# haven_point = Point(haven_locations.loc['AMALIA']['long'],haven_locations.loc['AMALIA']['lat'])\n",
    "\n",
    "# trayectory_AM, sp = trayectory_line(for_line, 'AMALIA', 20, 15000, haven_locations)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_map = folium.Map(\n",
    "    location=[51.9, 4.2],    \n",
    "    zoom_start=11\n",
    ")\n",
    "\n",
    "folium.features.GeoJson(trayectories).add_to(poly_map) \n",
    "\n",
    "poly_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapelysmooth import taubin_smooth\n",
    "from shapelysmooth import chaikin_smooth\n",
    "\n",
    "\n",
    "trayectories_smooth = trayectories.copy()\n",
    "\n",
    "for index, row in trayectories_smooth.iterrows():\n",
    "    original_geometry = row['geometry']\n",
    "    smoothed_geometry = chaikin_smooth(original_geometry,iters=5,keep_ends= True)\n",
    "    trayectories_smooth.at[index, 'smoothed_geometry'] = smoothed_geometry\n",
    "\n",
    "trayectories_smooth.set_geometry('smoothed_geometry', inplace=True)\n",
    "\n",
    "trayectories_smooth.set_crs(target_crs, inplace=True)\n",
    "\n",
    "poly_map = folium.Map(\n",
    "    location=[51.9, 4.2],    \n",
    "    zoom_start=11\n",
    ")\n",
    "\n",
    "folium.features.GeoJson(trayectories_smooth['smoothed_geometry']).add_to(poly_map) \n",
    "folium.features.GeoJson(trayectories).add_to(poly_map) \n",
    "\n",
    "poly_map\n",
    "\n",
    "#trayectories_smooth.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the order of the B-spline (typically cubic, i.e., order=3).\n",
    "order = 5\n",
    "\n",
    "# Create a parameter vector.\n",
    "t = np.linspace(0, 1, len(centroids) - order + 2, endpoint=True)\n",
    "\n",
    "# Create the B-spline curve.\n",
    "spline = BSpline(t, centroids, order)\n",
    "\n",
    "# Generate a finer set of points on the B-spline curve for plotting.\n",
    "finer_points = np.linspace(0, 1, 100)\n",
    "spline_points = spline(finer_points)\n",
    "\n",
    "# Plot the B-spline curve and control points.\n",
    "plt.plot(spline_points[:, 0], spline_points[:, 1], label=\"B-spline\")\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1,], c=\"red\", label=\"Control Points\")\n",
    "plt.legend()\n",
    "plt.title(\"B-spline Interpolation\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a LineString for each cluster\n",
    "trajectories = []\n",
    "for cluster_id in range(num_clusters):\n",
    "    cluster_points = for_line[for_line['cluster'] == cluster_id]['geometry'].values\n",
    "    cluster_coords = [point.coords[0] for point in cluster_points]\n",
    "\n",
    "    # Apply PCA to the cluster points\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(cluster_coords)\n",
    "    pca_points = pca.transform(cluster_coords)\n",
    "\n",
    "    # Create a LineString from the PCA-transformed points\n",
    "    trajectory = LineString(pca_points.tolist())\n",
    "    trajectories.append(trajectory)\n",
    "\n",
    "# Create a new GeoDataFrame with the resulting LineStrings\n",
    "line_gdf = gpd.GeoDataFrame({'geometry': trajectories})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming you have a GeoDataFrame 'gdf' with a 'geometry' column containing Point objects\n",
    "points = np.array([(point.y, point.x) for point in for_line['geometry']])\n",
    "tree = cKDTree(points)\n",
    "\n",
    "def mls_smooth(point, radius):\n",
    "    indices = tree.query_ball_point(point, r=radius)\n",
    "    local_points = points[indices]\n",
    "    return np.mean(local_points, axis=0)\n",
    "\n",
    "# Define a radius for MLS smoothing\n",
    "radius = 1 # Adjust this based on your data\n",
    "\n",
    "smoothed_points = [mls_smooth(point, radius) for point in points]\n",
    "\n",
    "# Convert the smoothed points back to Shapely Points\n",
    "smoothed_geometry = [Point(coord[0], coord[1]) for coord in smoothed_points]\n",
    "\n",
    "line = LineString(smoothed_geometry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now possess two primary data frames, namely \"leaving_data\" and \"incoming_data,\" which will serve as the foundation for our prediction model(s). These data frames have been meticulously processed and optimized, ensuring their readiness for the most efficient utilization in our analyses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can find below the visualization of a haven and all towages of the incoming tugs (tugs going to this haven). If wanted, it is also possible to do the same for each haven by changing the variable \"selected_haven\". Furthermore, by remplacing the variable \"tugs_incoming_gdf\" by \"tugs_leaving_gdf\", you can the observed the situation for the leaving tugs of a special haven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_haven = '6PET'\n",
    "\n",
    "\n",
    "# Create a Folium Map\n",
    "poly_map = folium.Map(\n",
    "    location=[51.9, 4.2],\n",
    "    zoom_start=11\n",
    ")\n",
    "\n",
    "# Filter the GeoDataFrame for markers with 'to_haven' equal to '7PET'\n",
    "heaven_gdf = tugs_incoming_gdf[tugs_incoming_gdf['to_haven'] == selected_haven]\n",
    "\n",
    "\n",
    "filtered_grid = gpd.sjoin(grid_within_polygon, heaven_gdf, how=\"inner\", op=\"intersects\")\n",
    "\n",
    "#filtered_grid.drop_duplicates(subset='geometry', keep='first', inplace=True, ignore_index=False)\n",
    "\n",
    "folium.features.GeoJson(filtered_grid['geometry']).add_to(poly_map) \n",
    "\n",
    "# Add CircleMarker for each point in the heaven GeoDataFrame\n",
    "for index, row in heaven_gdf.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=(row['geometry'].y, row['geometry'].x),  # Swap lat and lon\n",
    "        radius=3,  # Adjust the radius as needed for the size of the dot\n",
    "        color='red',  # Dot color\n",
    "        fill=True,\n",
    "        fill_color='red',  # Dot fill color\n",
    "        fill_opacity=0.6,\n",
    "    ).add_to(poly_map)\n",
    "    \n",
    "line.add_to(poly_map) \n",
    "\n",
    "\n",
    "# Display the map\n",
    "poly_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, our selection of variables for this study includes length, width, draught, wind speed, wind direction, harbour (to or from, whether the vessel is leaving or incoming), vessel type, and the status of being \"leaving or incoming.\" These variables were chosen based on both data-driven analysis and our intuition. However, we can't exactlyknow if each variables really plays a role or not. Thus, we believe these factors are crucial within the dataset for the following reasons:\n",
    "\n",
    "1. Boat dimensions (length, width): The size of the boat could impact the requirement for tugs. Larger vessels might require more assistance, and their dimensions can affect the duration and complexity of the process.\n",
    "\n",
    "2. Draught: The draught of the boat, indicating how much of it is submerged in water, can influence navigation. Different draught levels might require different tug assistance, especially in terms of turning and maneuvering.\n",
    "\n",
    "3. Wind conditions (speed, direction): Rotterdam's windy environment is a vital consideration. The flat terrain offers no natural windbreaks, potentially making wind a significant factor. Wind speed and direction can greatly affect a boat's ability to navigate, making it an essential variable for the need of tugboat.\n",
    "\n",
    "4. Harbour (leaving or incoming): Whether the vessel is leaving or incoming determines its starting and ending points. This information is fundamental for understanding where tugs are needed, considering departure and arrival locations.\n",
    "\n",
    "5. Vessel type: Different types of boats have varying capabilities. Some boats might be more adept at navigating challenging conditions, requiring less tug assistance, while others might need more support. Understanding the vessel type provides crucial insights into its self-navigating abilities.\n",
    "\n",
    "These variables were chosen using a combination of data analysis and our intuition, which comes from familiarity with maritime operations, via articles, movies, etc. By taking these things into account, we aim to gain a comprehensive understanding of the vessels pick-ups and drop-offs in Rotterdam, ensuring effective and safe navigation in this windy and challenging maritime environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizations on the final dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_colors = ['mediumslateblue', 'goldenrod', 'darkgrey']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representation in % of the different types of vessels in the leaving dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique vessel types\n",
    "vessel_type_counts = leaving_data['Type'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.pie(vessel_type_counts, autopct='%1.1f%%', startangle=140, colors=custom_colors)\n",
    "plt.title('Vessel type distribution (%)')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(vessel_type_counts.index, title='Vessel types', loc='best')\n",
    "plt.axis('equal')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representation in % of the different types of vessels in the incoming dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique vessel types\n",
    "vessel_type_counts = incoming_data['Type'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.pie(vessel_type_counts, autopct='%1.1f%%', startangle=140, colors=custom_colors)\n",
    "plt.title('Vessel type distribution (%)')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(vessel_type_counts.index, title='Vessel types', loc='best')\n",
    "plt.axis('equal')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizations of the dimensions and their quantiles in the leaving dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2x2 grid of subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 20))\n",
    "\n",
    "# List of dimensions\n",
    "dimensions = ['Lenght', 'Width', 'Navigation_draught']\n",
    "\n",
    "# Iterate over dimensions and corresponding subplots\n",
    "for dimension, ax in zip(dimensions, axes.flatten()):\n",
    "    sns.boxplot(data=leaving_data, x='Type', y=dimension, palette=custom_colors, ax=ax)\n",
    "    ax.set_xlabel('Vessel type')\n",
    "    ax.set_ylabel(f'Device dimensions to {dimension.split(\"_\")[-1].title()}')\n",
    "    \n",
    "    # Calculate and display statistical properties\n",
    "    for vessel_type in leaving_data['Type'].unique():\n",
    "        subset_data = leaving_data[leaving_data['Type'] == vessel_type]\n",
    "        median = subset_data[dimension].median()\n",
    "        q1 = subset_data[dimension].quantile(0.25)\n",
    "        q3 = subset_data[dimension].quantile(0.75)\n",
    "        max_val = subset_data[dimension].max()\n",
    "    \n",
    "\n",
    "# Set a single title for all subplots\n",
    "fig.suptitle('Dimensions', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizations of the dimensions and their quantiles in the incoming dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2x2 grid of subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 20))\n",
    "\n",
    "# List of dimensions\n",
    "dimensions = ['Lenght', 'Width', 'Navigation_draught']\n",
    "\n",
    "# Iterate over dimensions and corresponding subplots\n",
    "for dimension, ax in zip(dimensions, axes.flatten()):\n",
    "    sns.boxplot(data=incoming_data, x='Type', y=dimension, palette=custom_colors, ax=ax)\n",
    "    ax.set_xlabel('Vessel type')\n",
    "    ax.set_ylabel(f'Device dimensions to {dimension.split(\"_\")[-1].title()}')\n",
    "    \n",
    "    # Calculate and display statistical properties\n",
    "    for vessel_type in incoming_data['Type'].unique():\n",
    "        subset_data = incoming_data[incoming_data['Type'] == vessel_type]\n",
    "        median = subset_data[dimension].median()\n",
    "        q1 = subset_data[dimension].quantile(0.25)\n",
    "        q3 = subset_data[dimension].quantile(0.75)\n",
    "        max_val = subset_data[dimension].max()\n",
    "    \n",
    "\n",
    "# Set a single title for all subplots\n",
    "fig.suptitle('Dimensions', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code that we used before using the special tool. The code was running and working. However, we also made some changes and took some design decisions about the variables in this tool only. Indeed, the running time when it came to load more than one month became way too high. So we decided to use Alteryx Designer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def flatten_dict(d, parent_key='', sep='_'):\n",
    "#     items = {}\n",
    "#     for k, v in d.items():\n",
    "#         new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "#         if isinstance(v, dict):\n",
    "#             items.update(flatten_dict(v, new_key, sep=sep))\n",
    "#         else:\n",
    "#             items[new_key] = v\n",
    "#     return items\n",
    "\n",
    "\n",
    "# data = pd.DataFrame()\n",
    "# for i in range(6,13):\n",
    "#     # Opening JSON file\n",
    "#     f = open('Rotterdam_data/ais_rotterdam/ais_rotterdam_'+str(i)+'.json')\n",
    "    \n",
    "#     # returns JSON object as a dictionary\n",
    "#     json_data = json.load(f)\n",
    "    \n",
    "#     # Flatten each dictionary and store the results in a list\n",
    "#     flattened_dicts = [flatten_dict(d) for d in json_data['data']]\n",
    "    \n",
    "#     # Create a DataFrame\n",
    "#     data = pd.concat([data, pd.DataFrame(flattened_dicts)])\n",
    "\n",
    "# # Closing file\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #data['navigation_destination_eta'] = pd.to_datetime(data['navigation_destination_eta'])\n",
    "# #data['navigation_time'] = pd.to_datetime(data['navigation_time'])\n",
    "# # Remove rows where 'vessel_type' is 'tug'\n",
    "# data = data[data['vessel_type'] != 'tug']\n",
    "# # Remove the 'vessel_subtype' column\n",
    "# data = data.drop('vessel_subtype', 'vessel_callsign','navigation_status', 'navigation_destination_eta', 'navigation_heading', 'navigation_time', 'navigation_course', 'navigation_speed\t', 'navigation_location_type', 'navigation_location_coordinates')\n",
    "# #data[['navigation_location_lat', 'navigation_location_long']] = data['navigation_location_coordinates'].apply(lambda x: pd.Series(str(x).strip('[]').split(','))).astype(float)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_to_drop = [\n",
    "#     'vessel_callsign',\n",
    "#     'navigation_heading',\n",
    "#     'navigation_course',\n",
    "#     'navigation_speed',\n",
    "#     'navigation_location_type',\n",
    "# ]\n",
    "\n",
    "# numeric_columns = [\n",
    "#     'device_mmsi',\n",
    "#     'device_dimensions_to_bow',           \n",
    "#     'device_dimensions_to_stern',       \n",
    "#     'device_dimensions_to_starboard',     \n",
    "#     'device_dimensions_to_port',\n",
    "#     #'vessel_imo'\n",
    "#     'navigation_draught',\n",
    "#     #'vessel_type',\n",
    "#     #'vessel_subtype'\n",
    "# ]\n",
    "\n",
    "# info_columns = [\n",
    "#     'device_mmsi',\n",
    "#     'vessel_type',\n",
    "#     #'vessel_subtype'\n",
    "# ]\n",
    "\n",
    "# vessel_size = data[numeric_columns]\n",
    "# vessel_size = vessel_size.groupby('device_mmsi').median()\n",
    "# vessel_size['length'] = vessel_size['device_dimensions_to_bow'] + vessel_size['device_dimensions_to_stern']\n",
    "# vessel_size['width'] = vessel_size['device_dimensions_to_starboard'] + vessel_size['device_dimensions_to_port']\n",
    "# vessel_size.drop(['device_dimensions_to_bow','device_dimensions_to_stern', 'device_dimensions_to_starboard' ,'device_dimensions_to_port'], axis=1, inplace=True)\n",
    "# vessel_size.reset_index(inplace=True)\n",
    "\n",
    "# vessel_info = data[info_columns]  \n",
    "\n",
    "# vessel_info = vessel_info.groupby('device_mmsi').agg(lambda x: x.mode(0))\n",
    "# vessel_info.reset_index(inplace=True)\n",
    "\n",
    "# vessel_data = pd.merge(vessel_size,vessel_info)\n",
    "# vessel_data['vessel_type_Code'], unique_values = pd.factorize(vessel_data['vessel_type'])\n",
    "# main_db = pd.merge(tugs_dfb,vessel_data,how= 'right',left_on='vessel_mmsi',right_on='device_mmsi')\n",
    "# main_db.dropna(inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIEM6302",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
