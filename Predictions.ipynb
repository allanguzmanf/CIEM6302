{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kotug Project, Group 1\n",
    "\n",
    "Allan Guzman, June Chen, Sarah Blanc\n",
    "\n",
    "(Please make sure to first read the file called \"README.md\". It helps to get the main story line of what we did and the organization of the files.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's maybe first remember here the research questions of this project:\n",
    "\n",
    "## **Main Research Question**:\n",
    "\n",
    "\"How can the reliability and efficiency of Kotug OptiPort's tugboat scheduling tool be enhanced through the development and evaluation of novel methods for predicting pickup and drop-off coordinates and other relevant variables?\"\n",
    "\n",
    "## **Subresearch Questions**:\n",
    "\n",
    "- \"What advanced predictive methods can be developed and implemented to accurately forecast the coordinates (latitude and longitude) for pickup and drop-off locations of tugs on vessels in the port of Rotterdam?\"\n",
    "- \"Which variables, including AIS data, customer requirements, and port-specific constraints (or lack thereof), play a significant role in determining the pickup and drop-off locations and timing of tugs, as well as the required number of tugs for each vessel operation in the port of Rotterdam, and why are they important?\"\n",
    "- \"Which of the above mentioned variables can be combined in order to increase the reliability of the results, and why can they be combined?\"\n",
    "\n",
    "The work on this first research question in done in this file, while the 2 other subquestions are answered in the file called \"Data_preparation\". Finally, this file also contains the conclusion of this project (see end of the file).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Literature review that explains the path followed through the different methods used:\n",
    "\n",
    "As part of our effort to answer the first research question and to identify appropriate techniques for geospatial data processing, we painstakingly carried out a literature research. There were two goals in mind: finding viable approaches and understanding how they may be used in the field. Our investigation produced important findings that shaped our methodology. The key findings were the following:\n",
    "1. Integrated prediction over isolated coordinates: We could have been tempted at first to use several regression models to predict latitude and longitude independently. The literature, however, strongly advises against this practise. Researchers and academics often advise against this segmentation. This advice is supported by the fact that latitude and longitude have complex interactions that are difficult to fully comprehend when viewed as separate concepts. As a result, we began to concentrate on holistic approaches that take into account all of these factors at once.\n",
    "2. The potential of decision tree classifiers on our discretized polygon of water: Among the methods for classifying our data that showed promise was the decision tree classifier, especially when it came to pick-up and drop-off location prediction. It is not without restrictions, despite its potential. Indeed, we could use it, however we should create a dependence between squares (notable squares that are close to each other as an example). This could be done via new variables that we do not have. Developping them would be too complicated to apply in this project. Furthermore, its failure to adequately incorporate the geospatial nuances of our scenario is a major limitation. This insight drove us to read more in the literature and look for techniques that include the geographical dimension from the beginning. However, you can still see the results from this first method below.  \n",
    "3.  Introducing kriging, a spatial regression method: Our research introduced us to the Kriging technique, which is a very relevant method for our needs regarding the classification of geospatial data. This could be done on our discretized polygon of water. Furthermore, it is a reliable framework for position prediction that takes into consideration the underlying spatial patterns. Using the spatial correlations found in our data, this approach solves our categorization problem in a way that is contextually rich and nuanced. Unfortunately, after reading a lot about it, we again remarked that this method asks for a lot of complicated work before applying the method. We do not have the knowledges now to handle it in a way that allows us to produce convincing results. Thus, we decided to search for further possible methods.\n",
    "4. We reached the consensus that it's essential to reconsider approaches that don't rely on our discretized polygon. This broader perspective ensures we're not confined by our initial decision to go for this polygon. Following a brainstorming session and further reading of papers, we determined that ...... TO BE COMPLETED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sn\n",
    "\n",
    "import folium\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from geopy.distance import geodesic\n",
    "import folium\n",
    "\n",
    "from shapely.geometry import Point, Polygon, LineString\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from pyproj import Geod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_weather_size_draught = gpd.read_file('Data/Input_ML/incoming_weather_size_draught.geojson')\n",
    "incoming_weather_size = gpd.read_file('Data/Input_ML/incoming_weather_size.geojson')\n",
    "incoming_weather = gpd.read_file('Data/Input_ML/incoming_weather.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "havens = list(incoming_weather_size_draught['to_haven'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 5466 entries, 0 to 5465\n",
      "Data columns (total 19 columns):\n",
      " #   Column               Non-Null Count  Dtype              \n",
      "---  ------               --------------  -----              \n",
      " 0   area_ID              5466 non-null   int64              \n",
      " 1   center               5466 non-null   object             \n",
      " 2   from                 5466 non-null   object             \n",
      " 3   from_rounded         5466 non-null   datetime64[ns, UTC]\n",
      " 4   to_haven             5466 non-null   object             \n",
      " 5   trip_ID              5466 non-null   int64              \n",
      " 6   vessel_mmsi          5466 non-null   int64              \n",
      " 7   area                 5466 non-null   object             \n",
      " 8   Wind_Direction_Cat1  5466 non-null   object             \n",
      " 9   Wind_Direction_Cat2  5466 non-null   object             \n",
      " 10  Wind_Speed           5466 non-null   int64              \n",
      " 11  d_from_haven         5466 non-null   float64            \n",
      " 12  d_to_haven           5466 non-null   float64            \n",
      " 13  Wind_Direction       5466 non-null   int64              \n",
      " 14  Lenght               5466 non-null   float64            \n",
      " 15  Width                5466 non-null   float64            \n",
      " 16  Type                 5466 non-null   object             \n",
      " 17  Type_Cat             5466 non-null   int64              \n",
      " 18  geometry             5466 non-null   geometry           \n",
      "dtypes: datetime64[ns, UTC](1), float64(4), geometry(1), int64(6), object(7)\n",
      "memory usage: 811.5+ KB\n"
     ]
    }
   ],
   "source": [
    "incoming_weather_size.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaving_weather_size_draught = gpd.read_file('Data/Input_ML/leaving_weather_size_draught.geojson')\n",
    "leaving_weather_size = gpd.read_file('Data/Input_ML/leaving_weather_size.geojson')\n",
    "leaving_weather = gpd.read_file('Data/Input_ML/leaving_weather.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "RangeIndex: 2302 entries, 0 to 2301\n",
      "Data columns (total 20 columns):\n",
      " #   Column               Non-Null Count  Dtype              \n",
      "---  ------               --------------  -----              \n",
      " 0   area_ID              2302 non-null   int64              \n",
      " 1   center               2302 non-null   object             \n",
      " 2   to                   2302 non-null   object             \n",
      " 3   to_rounded           2302 non-null   datetime64[ns, UTC]\n",
      " 4   from_haven           2302 non-null   object             \n",
      " 5   trip_ID              2302 non-null   int64              \n",
      " 6   vessel_mmsi          2302 non-null   int64              \n",
      " 7   area                 2302 non-null   object             \n",
      " 8   Wind_Direction_Cat1  2302 non-null   object             \n",
      " 9   Wind_Direction_Cat2  2302 non-null   object             \n",
      " 10  Wind_Speed           2302 non-null   int64              \n",
      " 11  d_from_haven         2302 non-null   float64            \n",
      " 12  d_to_haven           2302 non-null   float64            \n",
      " 13  Wind_Direction       2302 non-null   int64              \n",
      " 14  Lenght               2302 non-null   float64            \n",
      " 15  Width                2302 non-null   float64            \n",
      " 16  Type                 2302 non-null   object             \n",
      " 17  Type_Cat             2302 non-null   int64              \n",
      " 18  Navigation_draught   2302 non-null   float64            \n",
      " 19  geometry             2302 non-null   geometry           \n",
      "dtypes: datetime64[ns, UTC](1), float64(5), geometry(1), int64(6), object(7)\n",
      "memory usage: 359.8+ KB\n"
     ]
    }
   ],
   "source": [
    "leaving_weather_size_draught.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Trajectories approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that will be used for the predictions\n",
    "columns_leaving_trajectories = [\n",
    "    'from_haven',\n",
    "    'Wind_Direction',\n",
    "    'Wind_Speed',\n",
    "    'Lenght',\n",
    "    'Width',\n",
    "    'Type_Cat',\n",
    "    'd_from_haven'\n",
    "]\n",
    "\n",
    "columns_incoming_trajectories = [\n",
    "    'to_haven',\n",
    "    'Wind_Direction',\n",
    "    'Wind_Speed',\n",
    "    'Lenght'\t,\n",
    "    'Width'\t,\n",
    "    'Type_Cat',\n",
    "    'd_to_haven'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Regressor is an ensemble learning method based on the decision tree algorithm. It operates by constructing multiple decision trees during training and outputs the average prediction of the individual trees for regression tasks. In the context of assigning pick-up and drop-off locations to vessels following a trajectory, using a Random Forest Regressor offers several advantages:\n",
    "1. Handling non-linearity: The random forest method can capture complex, nonlinear relationships in the data, making them suitable for predicting pick-up and drop-off locations;\n",
    "2. Ensemble learning: Random Forest is an ensemble learning technique, meaning it combines predictions from multiple individual decision trees. This ensemble approach often leads to more accurate and stable predictions compared to a single decision tree. \n",
    "3. Robustness to Overfitting: Random Forests are inherently resistant to overfitting, a common challenge in complex modeling tasks. By aggregating predictions from multiple trees, it tends to generalize better to unseen data, ensuring robust and reliable predictions for vessel locations.\n",
    "4. Feature importance: this method provides a natural way to rank the importance of input features. Understanding which variables significantly influence pick-up and drop-off locations is crucial for decision-making.\n",
    "5. Handling multiple variables: Random Forests can handle diverse types of data, including numerical and categorical variables. \n",
    "6. Outlier resistance: This method is less sensitive to outliers due to the averaging effect from multiple trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for LEAVING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_incoming = incoming_weather_size_draught[columns_incoming_trajectories]\n",
    "corr_matrix_incoming = corr_matrix_incoming.drop(columns = 'to_haven').corr()\n",
    "sn.heatmap(corr_matrix_incoming, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_leaving = leaving_weather_size_draught[columns_leaving_trajectories]\n",
    "corr_matrix_leaving = corr_matrix_leaving.drop(columns = 'from_haven').corr()\n",
    "sn.heatmap(corr_matrix_leaving, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_trajectories = incoming_weather_size_draught[columns_incoming_trajectories]\n",
    "incoming_trajectories = incoming_trajectories.groupby('to_haven').apply(lambda x: x).drop(columns = 'to_haven')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_forest(grouped_df, haven):\n",
    "    haven_df = grouped_df.loc[haven]\n",
    "\n",
    "    X = haven_df.drop(columns= 'd_to_haven')\n",
    "    y = haven_df['d_to_haven'] \n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create and train the Random Forest model\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "\n",
    "    # Add the predicted distances to the original DataFrame\n",
    "    X_test['predicted_distance'] = y_pred\n",
    "\n",
    "    X_test['original_distance'] = haven_df['d_to_haven'].loc[list(X_test.index.values)]\n",
    "    X_test['diff_dist'] = abs(X_test['predicted_distance']  - X_test['original_distance'])\n",
    "    \n",
    "    average_diff = X_test['diff_dist'].mean()\n",
    "\n",
    "    values = [average_diff]\n",
    "    columns = ['avg_diff']\n",
    "\n",
    "    # You can also access feature importances\n",
    "    feature_importances = rf_model.feature_importances_.tolist()\n",
    "\n",
    "    values.extend(feature_importances)\n",
    "    columns.extend(X.columns)\n",
    "\n",
    "    values.extend([haven])\n",
    "    columns.extend(['haven'])\n",
    "\n",
    "    output = pd.DataFrame([values], columns=columns)\n",
    "\n",
    "    return output, X_test\n",
    "\n",
    "# # Evaluate the model\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "# print(f\"R-squared (R2): {r2:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Now, your DataFrame 'df' contains a new column 'predicted_distance' with the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050.6315016069377\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_diff</th>\n",
       "      <th>Wind_Direction</th>\n",
       "      <th>Wind_Speed</th>\n",
       "      <th>Lenght</th>\n",
       "      <th>Width</th>\n",
       "      <th>Type_Cat</th>\n",
       "      <th>haven</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>861.254675</td>\n",
       "      <td>0.224437</td>\n",
       "      <td>0.259506</td>\n",
       "      <td>0.323764</td>\n",
       "      <td>0.192293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3PET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>690.717969</td>\n",
       "      <td>0.286619</td>\n",
       "      <td>0.303499</td>\n",
       "      <td>0.267335</td>\n",
       "      <td>0.129228</td>\n",
       "      <td>0.013319</td>\n",
       "      <td>PET8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>603.989579</td>\n",
       "      <td>0.107526</td>\n",
       "      <td>0.158392</td>\n",
       "      <td>0.535464</td>\n",
       "      <td>0.198618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>AMAZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>882.088754</td>\n",
       "      <td>0.143865</td>\n",
       "      <td>0.185903</td>\n",
       "      <td>0.475506</td>\n",
       "      <td>0.173219</td>\n",
       "      <td>0.021507</td>\n",
       "      <td>EURO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1374.674628</td>\n",
       "      <td>0.098096</td>\n",
       "      <td>0.166267</td>\n",
       "      <td>0.639018</td>\n",
       "      <td>0.095338</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>AMALIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1004.614557</td>\n",
       "      <td>0.075155</td>\n",
       "      <td>0.107247</td>\n",
       "      <td>0.503531</td>\n",
       "      <td>0.213919</td>\n",
       "      <td>0.100148</td>\n",
       "      <td>YANG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1426.486202</td>\n",
       "      <td>0.188461</td>\n",
       "      <td>0.240462</td>\n",
       "      <td>0.367820</td>\n",
       "      <td>0.183527</td>\n",
       "      <td>0.019730</td>\n",
       "      <td>CKVTTI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1363.921374</td>\n",
       "      <td>0.168008</td>\n",
       "      <td>0.203784</td>\n",
       "      <td>0.405744</td>\n",
       "      <td>0.221625</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>7PET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>766.504703</td>\n",
       "      <td>0.210797</td>\n",
       "      <td>0.260287</td>\n",
       "      <td>0.237094</td>\n",
       "      <td>0.259586</td>\n",
       "      <td>0.032236</td>\n",
       "      <td>MISS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1065.396091</td>\n",
       "      <td>0.204749</td>\n",
       "      <td>0.164258</td>\n",
       "      <td>0.334126</td>\n",
       "      <td>0.262006</td>\n",
       "      <td>0.034861</td>\n",
       "      <td>WAAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      avg_diff  Wind_Direction  Wind_Speed    Lenght     Width  Type_Cat  \\\n",
       "0   861.254675        0.224437    0.259506  0.323764  0.192293  0.000000   \n",
       "1   690.717969        0.286619    0.303499  0.267335  0.129228  0.013319   \n",
       "2   603.989579        0.107526    0.158392  0.535464  0.198618  0.000000   \n",
       "3   882.088754        0.143865    0.185903  0.475506  0.173219  0.021507   \n",
       "4  1374.674628        0.098096    0.166267  0.639018  0.095338  0.001282   \n",
       "5  1004.614557        0.075155    0.107247  0.503531  0.213919  0.100148   \n",
       "6  1426.486202        0.188461    0.240462  0.367820  0.183527  0.019730   \n",
       "7  1363.921374        0.168008    0.203784  0.405744  0.221625  0.000839   \n",
       "8   766.504703        0.210797    0.260287  0.237094  0.259586  0.032236   \n",
       "9  1065.396091        0.204749    0.164258  0.334126  0.262006  0.034861   \n",
       "\n",
       "    haven  \n",
       "0    3PET  \n",
       "1    PET8  \n",
       "2    AMAZ  \n",
       "3    EURO  \n",
       "4  AMALIA  \n",
       "5    YANG  \n",
       "6  CKVTTI  \n",
       "7    7PET  \n",
       "8    MISS  \n",
       "9    WAAL  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data frame to get an overview of the results and the feature importance\n",
    "output = []\n",
    "overall_avg = []\n",
    "for haven in havens:\n",
    "    avg_diff, pred = Random_forest(incoming_trajectories, haven)\n",
    "    overall_avg.extend(list(pred['diff_dist']))\n",
    "    output.append(avg_diff)\n",
    "\n",
    "# Overall Average distance between predicted and real locations\n",
    "print(\"Average distance between predicted and real locations: \", sum(overall_avg)/len(overall_avg), \"meters\")\n",
    "pd.concat(output).reset_index(drop= True)\n",
    "\n",
    "#print(sum(overall_avg)/len(overall_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "to_haven      \n",
       "3PET      0          0.000000\n",
       "          1        629.430921\n",
       "          23      1795.935183\n",
       "          24      1303.474140\n",
       "          29      2347.385090\n",
       "                     ...     \n",
       "YANG      2473    1154.566618\n",
       "          2483    1778.543152\n",
       "          2485       0.000000\n",
       "          2488     974.118683\n",
       "          2497    8628.836551\n",
       "Name: d_to_haven, Length: 2547, dtype: float64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trajectory_classification(grouped_df, haven):\n",
    "    haven_data = grouped_df.loc[haven]\n",
    "    X = haven_data.drop(columns= 'd_to_haven')\n",
    "    y = haven_data['d_to_haven']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for INCOMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Areas approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the variation in data availability across different harbors, this strategic approach was firstly adopted. We specifically targeted a single harbor for analysis. The flexibility of this model allows us to select any harbor for analysis simply by modifying a designated variable in the code. Opting for a specific harbor selection has the added advantage of narrowing down the relevant squares associated with the chosen harbor. This focused approach significantly reduces the number of squares considered in each case (only the squares that are relevant for each harbour), consequently decreasing the computational power required for the predictions.\n",
    "\n",
    "To elaborate, we begin by choosing a harbor with substantial data volume. All relevant data pertaining to incoming and departing vessels from this selected harbor is extracted. Our model is then applied to this subset of data. Notably, the ability to change the harbor variable in the code grants the flexibility to extend this analysis to other harbors seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from_haven\n",
      "EURO      451\n",
      "AMALIA    309\n",
      "Name: count, dtype: int64\n",
      "to_haven\n",
      "EURO    480\n",
      "7PET    377\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 2 most used harbour for the incoming and leaving vessels\n",
    "leaving_top = leaving_weather_size_draught['from_haven'].value_counts()\n",
    "incoming_top = incoming_weather_size_draught['to_haven'].value_counts()\n",
    "print(leaving_top[0:2])\n",
    "print(incoming_top[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of the data related to the most used habours for leaving and incoming vessels\n",
    "incoming_harbor = 'EURO'\n",
    "leaving_harbor = 'EURO'\n",
    "\n",
    "leaving_weather_size_draught_top = leaving_weather_size_draught[leaving_weather_size_draught['from_haven']==leaving_harbor]\n",
    "incoming_weather_size_draught_top = incoming_weather_size_draught[incoming_weather_size_draught['to_haven']==incoming_harbor]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that will ne used in this prediction model\n",
    "columns_leaving_areas = [\n",
    "    'Wind_Direction',\n",
    "    'Wind_Speed',\n",
    "    'Lenght',\n",
    "    'Width',\n",
    "    'Type',\n",
    "]\n",
    "\n",
    "columns_incoming_areas = [\n",
    "    'Wind_Direction',\n",
    "    'Wind_Speed',\n",
    "    'Lenght'\t,\n",
    "    'Width'\t,\n",
    "    'Type',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach, we decided to use Decision Tree Classifier here. This method is a supervised machine learning algorithm that can be used for both classification and regression tasks. In the context of classification, like assigning pick-up and drop-off locations to vessels in our discretized polygon, a Decision Tree Classifier works by recursively partitioning the dataset into subsets based on the input features. The advantages of this method are that it can handle mixed data types such as categorical and numerical data; it can handle non-linearity; and it can provide the feature importance by identifying which features significantly influence the assignment of pick-up and drop-off points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for LEAVING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split of the data into training and testing sets\n",
    "X = leaving_weather_size_draught_top[columns_leaving_areas]\n",
    "y = leaving_weather_size_draught_top['area_ID']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=3)\n",
    "data = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Decision Tree Classifier: 19.03%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=3)\n",
    "\n",
    "# Train the classifier using the training data\n",
    "x = decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = decision_tree_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy of the Decision Tree Classifier: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Add predictions to the original DataFrame\n",
    "\n",
    "data['Predicted_Area_ID'] = predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the displayed accuracy appears relatively low, it's essential to consider the context. Kotug provided a specific goal of achieving approximately 500 meters of accuracy. Therefore, the percentage accuracy alone does not provide the most pertinent insight into the model's efficiency. To gain a more meaningful assessment, we will calculate the disparity in meters between the predicted points and the actual pick-up and drop-off locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(data, leaving_weather_size_draught, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new empty list to store prediction_center values\n",
    "prediction_centers = []\n",
    "\n",
    "# Iterate through the rows of merged_df\n",
    "for index, row in merged_df.iterrows():\n",
    "    # Get the Predicted_Area_ID value from the current row of merged_df\n",
    "    predicted_area_id = row['Predicted_Area_ID']\n",
    "    \n",
    "    # Find the corresponding row in incoming_weather_size_draught where area_ID matches Predicted_Area_ID\n",
    "    matching_row = leaving_weather_size_draught[leaving_weather_size_draught['area_ID'] == predicted_area_id]\n",
    "    \n",
    "    # Check if a matching row is found\n",
    "    if not matching_row.empty:\n",
    "        # Get the value from the 'center' column in the matching row\n",
    "        prediction_center = matching_row['center'].values[0]\n",
    "    else:\n",
    "        # If no matching row is found, assign a default value (you can change this as per your requirement)\n",
    "        prediction_center = None\n",
    "    \n",
    "    # Append the prediction_center value to the list\n",
    "    prediction_centers.append(prediction_center)\n",
    "\n",
    "# Add the prediction_centers list as a new column to merged_df\n",
    "merged_df['prediction_center'] = prediction_centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'to_location'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/CIEM6302/lib/python3.10/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3790\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3791\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'to_location'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h2/p_nmk5y57h30_tgf8ch2knm80000gn/T/ipykernel_972/3897079508.py\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Convert 'from_location' and 'center' columns to actual coordinates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'to_location_coords'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'to_location'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_coordinates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'center_coords'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction_center'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_coordinates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/CIEM6302/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3894\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3896\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3897\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3898\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/CIEM6302/lib/python3.10/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3795\u001b[0m             ):\n\u001b[1;32m   3796\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3797\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3798\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3799\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'to_location'"
     ]
    }
   ],
   "source": [
    "# Function to convert string representation of coordinates to actual coordinates\n",
    "def parse_coordinates(coord_str):\n",
    "    if coord_str:\n",
    "        # Extract numerical values from the string (format: 'POINT (longitude latitude)')\n",
    "        coordinates = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", coord_str)\n",
    "        lat, lon = map(float, coordinates)\n",
    "        return lat, lon\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Convert 'from_location' and 'center' columns to actual coordinates\n",
    "merged_df['to_location_coords'] = merged_df['to_location'].apply(parse_coordinates)\n",
    "merged_df['center_coords'] = merged_df['prediction_center'].apply(parse_coordinates)\n",
    "\n",
    "# Function to calculate distance between two points using geodesic\n",
    "def calculate_distance(row):\n",
    "    if row['to_location_coords'] and row['center_coords']:\n",
    "        return geodesic(row['to_location_coords'], row['center_coords']).meters\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the calculate_distance function to the DataFrame to compute distances\n",
    "merged_df['distance_to_predicted_center'] = merged_df.apply(calculate_distance, axis=1)\n",
    "moyenne_distance = merged_df['distance_to_predicted_center'].mean()\n",
    "\n",
    "# Afficher la moyenne\n",
    "print(\"Average distance between predicted and real locations: \", moyenne_distance, \"meters\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In alignment with the findings outlined in the literature review, the obtained result falls significantly short of accuracy expectations. The targeted goal set by Kotug of achieving a 500-meter accuracy remains distant. A notable factor contributing to this imprecision is the lack of interdependence observed between the various squares in the analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions for INCOMING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = incoming_weather_size_draught_top[columns_incoming_areas]\n",
    "y = incoming_weather_size_draught_top['area_ID']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=3)\n",
    "#X_from_lat_train, X_from_lat_test, y_from_lat_train, y_from_lat_test = train_test_split(leaving_weather_size_draught[columns_leaving], leaving_weather_size_draught['area_ID'], test_size=0.3, random_state=42)\n",
    "data = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Decision Tree Classifier: 12.92%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Direction</th>\n",
       "      <th>Wind_speed</th>\n",
       "      <th>Lenght</th>\n",
       "      <th>Width</th>\n",
       "      <th>Type</th>\n",
       "      <th>Predicted_Area_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>299.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>300.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2258</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>366.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>270.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>300.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>300.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3032</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>300.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>199.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Direction  Wind_speed  Lenght  Width  Type  Predicted_Area_ID\n",
       "219           2           1   299.0   49.0     0               1224\n",
       "772           5           3   300.0   40.0     0               1027\n",
       "600           8           0   243.0   32.0     0               1103\n",
       "2258          7           1   366.0   48.0     0               1067\n",
       "2033          5           0   333.0   48.0     0               1107\n",
       "...         ...         ...     ...    ...   ...                ...\n",
       "407           7           1   270.0   40.0     0               1106\n",
       "1780          5           3   300.0   40.0     0               1027\n",
       "1224          5           3   300.0   43.0     0               1106\n",
       "3032          6           1   300.0   48.0     0               1186\n",
       "1130          8           1   199.0   30.0     0               1107\n",
       "\n",
       "[240 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier(random_state=3)\n",
    "\n",
    "# Train the classifier using the training data\n",
    "x = decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = decision_tree_classifier.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(\"Accuracy of the Decision Tree Classifier: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Add predictions to the original DataFrame\n",
    "\n",
    "data['Predicted_Area_ID'] = predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, while the displayed accuracy appears relatively low, it's essential to consider the context. Kotug provided a specific goal of achieving approximately 500 meters of accuracy. Therefore, the percentage accuracy alone does not provide the most pertinent insight into the model's efficiency. To gain a more meaningful assessment, we will calculate the disparity in meters between the predicted points and the actual pick-up and drop-off locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(data, incoming_weather_size_draught, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new empty list to store prediction_center values\n",
    "prediction_centers = []\n",
    "\n",
    "# Iterate through the rows of merged_df\n",
    "for index, row in merged_df.iterrows():\n",
    "    # Get the Predicted_Area_ID value from the current row of merged_df\n",
    "    predicted_area_id = row['Predicted_Area_ID']\n",
    "    \n",
    "    # Find the corresponding row in incoming_weather_size_draught where area_ID matches Predicted_Area_ID\n",
    "    matching_row = incoming_weather_size_draught[incoming_weather_size_draught['area_ID'] == predicted_area_id]\n",
    "    \n",
    "    # Check if a matching row is found\n",
    "    if not matching_row.empty:\n",
    "        # Get the value from the 'center' column in the matching row\n",
    "        prediction_center = matching_row['center'].values[0]\n",
    "    else:\n",
    "        # If no matching row is found, assign a default value (you can change this as per your requirement)\n",
    "        prediction_center = None\n",
    "    \n",
    "    # Append the prediction_center value to the list\n",
    "    prediction_centers.append(prediction_center)\n",
    "\n",
    "# Add the prediction_centers list as a new column to merged_df\n",
    "merged_df['prediction_center'] = prediction_centers\n",
    "\n",
    "#merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average distance between predicted and real locations:  1418.1801243934678 meters\n"
     ]
    }
   ],
   "source": [
    "# Function to convert string representation of coordinates to actual coordinates\n",
    "def parse_coordinates(coord_str):\n",
    "    if coord_str:\n",
    "        # Extract numerical values from the string (format: 'POINT (longitude latitude)')\n",
    "        coordinates = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", coord_str)\n",
    "        lat, lon = map(float, coordinates)\n",
    "        return lat, lon\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Convert 'to_location' and 'center' columns to actual coordinates\n",
    "merged_df['from_location_coords'] = merged_df['from_location'].apply(parse_coordinates)\n",
    "merged_df['center_coords'] = merged_df['prediction_center'].apply(parse_coordinates)\n",
    "\n",
    "# Function to calculate distance between two points using geodesic\n",
    "def calculate_distance(row):\n",
    "    if row['from_location_coords'] and row['center_coords']:\n",
    "        return geodesic(row['from_location_coords'], row['center_coords']).meters\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the calculate_distance function to the DataFrame to compute distances\n",
    "merged_df['distance_to_predicted_center'] = merged_df.apply(calculate_distance, axis=1)\n",
    "moyenne_distance = merged_df['distance_to_predicted_center'].mean()\n",
    "\n",
    "# Afficher la moyenne\n",
    "print(\"Average distance between predicted and real locations: \", moyenne_distance, \"meters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In alignment with the findings outlined in the literature review, the obtained result falls significantly short of accuracy expectations. The targeted goal set by Kotug of achieving a 500-meter accuracy remains distant. A notable factor contributing to this imprecision is the lack of interdependence observed between the various squares in the analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CONCLUSION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIEM6302",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
